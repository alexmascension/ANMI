{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexmascension/ANMI/blob/main/notebook/T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf3GioQwcepr"
   },
   "source": [
    "# Tema 6: Derivación e integración numérica\n",
    "\n",
    "Este tema lo construyo en parte basado en https://brianheinold.net/notes/An_Intuitive_Guide_to_Numerical_Methods_Heinold.pdf, que proporciona un enfoque bastante más práctico del tema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hDZNaYFX936"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/alexmascension/ANMI/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYo_JGn2TSxC"
   },
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "from sympy.matrices import Matrix as mat\n",
    "from sympy.matrices import randMatrix\n",
    "from sympy import symbols\n",
    "import sympy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.linalg import orth\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anmi.T5 import polinomio_newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-V6kF8_X937"
   },
   "outputs": [],
   "source": [
    "from anmi.genericas import norma_p_func, norma_inf_func, matriz_inversa\n",
    "from anmi.T5 import polinomio_lagrange, polinomio_newton, error_lagrange, error_maximo_estocastico, roots_chebyshev\n",
    "from anmi.T5 import aitken_neville, interpolacion_hermite, polinomio_generico, esplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, a, lambda_ = symbols('x'), symbols('y'), symbols('z'), symbols('a'), symbols('lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivación numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema de la aritmética finita en el cálculo de derivadas\n",
    "\n",
    "El problema principal del cálculo de derivadas está en que el denominador de la derivada se hace muy pequeño:\n",
    "$f'(x) = \\frac{f(x+h) - f(x)}{h}$\n",
    "\n",
    "En ese sentido, si empezamos a usar valores de $h$ muy pequeños, podemos acabar teniendo valores de derivada alejados de la realidad. Como ejemplo, vamos a evaluar la derivada de $2x$ en $x=3$, empleando diferentes valores de $h$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(23):\n",
    "    print(n, (((x+h) ** 2 - x**2) / (h)).subs(h, 10**(-n)).subs(x, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que para valores de h de $10^{-13}$ para abajo, las derivadas se nos van de madre. Esto es problemático porque para derivadas más complejas el error aumenta y no nos podremos fiar directamente de los resultados. Para mitigar estos problemas en estas secciones trabajaremos con diferentes métodos de aproximación de derivadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fórmulas de derivación numérica\n",
    "En esta sección vamos a derivar diferentes métodos de cálculo de derivadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el polinomio de Newton\n",
    "Una de las opciones para calcular la derivada $n$-ésima es generar un polinomio de grado $n$ y aplicar su derivada $n$ veces. Para ello podemos emplear el método de Newton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(polinomio_newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(derivacion_polinomio_newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivacion_polinomio_newton(f, x0, var=symbols('x'), grado=1, h=0.01):\n",
    "    x_vals = [x0 + i * h for i in range(grado + 1)]\n",
    "    y_vals = [f.subs(var, (x0 + i * h)) for i in range(grado + 1)]\n",
    "    \n",
    "    # Con esto creamos el polinomio\n",
    "    p, m = polinomio_newton(x_vals, y_vals, var)\n",
    "    \n",
    "    # Ahora lo diferenciamos\n",
    "    p_diff = p\n",
    "    for i in range(grado):\n",
    "        p_diff = p_diff.diff(var)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return p_diff, p, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EJEMPLO 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diff, p, m =  derivacion_polinomio_newton(f=sqrt(x), x0=1, var=symbols('x'), grado=3, h=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N(p_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a jugar con otros valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diff, p, m =  derivacion_polinomio_newton(f=sqrt(x), x0=1, var=symbols('x'), grado=3, h=0.01)\n",
    "N(p_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diff, p, m =  derivacion_polinomio_newton(f=sqrt(x), x0=1, var=symbols('x'), grado=3, h=0.001)\n",
    "N(p_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diff, p, m =  derivacion_polinomio_newton(f=sqrt(x), x0=1, var=symbols('x'), grado=3, h=0.0001)\n",
    "N(p_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con $h$ más pequeños, el valor se asemeja más al verdadero de 0.375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diff, p, m =  derivacion_polinomio_newton(f=sqrt(x), x0=1, var=symbols('x'), grado=3, h=0.0000001)\n",
    "N(p_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diff, p, m =  derivacion_polinomio_newton(f=sqrt(x), x0=1, var=symbols('x'), grado=3, h=0.000000001)\n",
    "N(p_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, como antes, si tomamos $h$ extremadamente pequeños, el cálculo se nos va de madre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de error en derivadas\n",
    "Muchas veces nos interesa poder hallar el error de aproximación de la derivada, para ver hasta qué punto nos parece una aproximación adecuada.\n",
    "Recordemos que la expansión de Taylor de $f(x)$ en $x=a$ es:\n",
    "$$f(x) = f(a) + f'(a)(x-a) + \\frac{f^{(2)}(a)}{2!}(x-a)^2 + \\frac{f^{(3)}(a)}{3!}(x-a)^3 + \\cdots$$\n",
    "Si sustituimos $x$ for $x+h$ y tomando $a$ como $x-h$, la fórmula anterior queda como:\n",
    "$$f(x+h) = f(x) + f'(x)h + \\frac{f^{(2)}(x)}{2!}h^2 + \\frac{f^{(3)}(x)}{3!}h^3 + \\cdots$$\n",
    "Resolviendo para $f'(x)$ tenemos:\n",
    "$$ f'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{f^{(2)}(x)}{2!}h - \\frac{f^{(3)}(x)}{3!}h^2 - \\cdots$$\n",
    "\n",
    "Luego el error de aproximación pertenecería a los términos de derivada segunda y superiores. Para simplificar, nos quedamos solo con el término de derivada segunda y se cumple que, para un $c \\in [x, x+h]$\n",
    "$$f'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{f^{(2)}(c)}{2!}h$$\n",
    "De modo que podemos acotar el error.\n",
    "\n",
    "Esta estrategia de cálculo de error la emplearemos después en otros métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivación con polinomios de Lagrange\n",
    "Otro método es construyendo polinomios de Lagrange. Recordemos que los polinomios vienen dados por la fórmula:\n",
    "$$p_n(x) = \\sum_{i=0}^n l_i(x)f(x_i)$$\n",
    "Si aplicamos una derivación a $p_n(x)$ en $\\bar{x}_0$ obtenemos el siguiente polinomio:\n",
    "$$D(f)(\\bar{x_0}) = \\sum_{i = 0}^n l'_i(\\bar{x}_0)f(x_i)$$\n",
    "$\\bar{x_0}$ es un punto cualquiera del intervalo, y puede o no coincidir con alguno de los nodos.\n",
    "\n",
    "Recordemos que la construcción de $p(x)$ viene dada por la base $\\{1, x, \\cdots, x^n\\}$ y cada termino es $x^j = \\sum_{i=0}^n l_i(x)x^j$, de modo que, matricialmente, $p(x)$ se construye como:\n",
    "$$\\begin{bmatrix}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "x_0 & x_1 & \\cdots & x_n\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_0^n & x_1^n & \\cdots & x_n^n\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "l_0\\\\\n",
    "l_1\\\\\n",
    "\\vdots\\\\\n",
    "l_n\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "x\\\\\n",
    "\\vdots\\\\\n",
    "x^n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Si derivamos $r$ veces en $\\bar{x}_0$, acabamos con el sistema:\n",
    "$$\\begin{bmatrix}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "x_0 & x_1 & \\cdots & x_n\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_0^n & x_1^n & \\cdots & x_n^n\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "l^{(r)}_0=a_0\\\\\n",
    "l^{(r)}_1=a_1\\\\\n",
    "\\vdots\\\\\n",
    "l^{(r)}_n=a_n\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "(n)(n-1)\\cdots(n-r)\\bar{x}_0^{n-r}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(polinomio_lagrange_derivada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implementar la función de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polinomio_lagrange_derivada(f, x0, x_vals=None, var=symbols('x'), grado=1, I=[0, 1]):\n",
    "    ff = Function('f')\n",
    "    \n",
    "    if x_vals is None:\n",
    "        x_vals = np.linspace(I[0], I[1], grado + 1)\n",
    "    assert len(x_vals) == grado + 1\n",
    "    \n",
    "    G = ones(len(x_vals), len(x_vals))\n",
    "    for row in range(1, len(x_vals)):\n",
    "        for col in range(len(x_vals)):\n",
    "            G[row, col] = x_vals[col] ** row\n",
    "            \n",
    "    x_mat = Matrix([var ** i for i in range(grado + 1)])\n",
    "    for _ in range(grado):\n",
    "        x_mat = x_mat.diff(var)\n",
    "    \n",
    "    # Con esto creamos el vector de valores de A\n",
    "    a = matriz_inversa(G) * x_mat\n",
    "    \n",
    "    p, val = S(0), S(0)\n",
    "    for i in range(grado + 1):\n",
    "        p += a[i] * ff(x_vals[i])\n",
    "        val += a[i] * f.subs(var, x_vals[i])\n",
    "    \n",
    "    return p, val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polinomio_lagrange_derivada(f=x**3, x0=2, x_vals=[1, 2, 3], var=symbols('x'), grado=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de extrapolación de Richardson\n",
    "Para explicar el método me voy a basar en la explicación de Wikipedia, que generaliza mejor, con cambios de notación. https://en.wikipedia.org/wiki/Richardson_extrapolation\n",
    "\n",
    "Supongamos que tenemos una función $N(h)$, que la empleamos como aproximación a la derivada en $h=0$, $N(0)$. La función $N(h)$ va a tener una expansión del error del siguiente modo:\n",
    "$$N(h) = N(0) + a_0h^{k_0} + a_1h^{k_1} + a_2h^{k_2} + \\cdots$$\n",
    "\n",
    "En primer lugar, tomamos un parametro $r$, de modo que podemos sustituir $h$ por $rh$ del siguiente modo:\n",
    "$$N(rh) = N(0) + a_0(rh)^{k_0} + a_1(rh)^{k_1} + a_2(rh)^{k_2} + \\cdots$$\n",
    "\n",
    "Si multiplicamos por $(r)^{k_0}$ y restamos tenemos que:\n",
    "$$r^{k_0}N(h) =  N(0) + a_0r^{k_0}h^{k_0} + a_1r^{k_0}h^{k_1} + a_2r^{k_0}h^{k_2} + \\cdots$$\n",
    "\n",
    "$$r^{k_0}N(h) -  N(rh)=  N(0) + (r^{k_0} - r^{k_1})a_1h^{k_1} + (r^{k_0} - r^{k_2})a_2h^{k_2} + \\cdots$$\n",
    "\n",
    "Luego\n",
    "$$N_1(h) = \\frac{r^{k_0}N(h) -  N(rh)}{r^{k_0} - 1}= N(0) + \\frac{r^{k_0} - r^{k_1}}{r^{k_0} - 1}a_1h^{k_1} + \\frac{r^{k_0} - r^{k_2}}{r^{k_0} - 1}a_2h^{k_2} + \\cdots$$\n",
    "\n",
    "Vemos que ahora tenemos una nueva función, $N_1(h)$, que depende de $a_1h^{k_1} + a_2h^{k_2} + \\cdots$, luego hemos reducido su error, ya que $\\frac{r^{k_0} - r^{k_1}}{r^{k_0} - 1}$ tiene menos influencia que el orden de $h$. Con iteraciones sucesivas podemos hacer este error tan pequeño como veamos, dentro de la capacidad de error aritmética de $r^nh$ para el ordenador.\n",
    "\n",
    "Generalizando, la fórmula final es:\n",
    "$$N_a(r^bh) = \\frac{r^{m+a-1}N(r^bh) - N(r^{b+1}h)}{r^{m+a-1} - 1}$$\n",
    "\n",
    "El valor $m=k_0$ depende de $N(h)$. Si, por ejemplo, $N(h) = \\frac{f(x+h) - f(x-h)}{2h}$, considerando los desarrollos de Taylor:\n",
    "$$f(x+h) = f(x) + f'(x)fh + \\frac{1}{2}f''(x)h^2 + \\frac{1}{3!}f'''(x)h^3 + \\frac{1}{4!}f''''(x)h^4 + \\cdots$$\n",
    "$$f(x-h) = f(x) - f'(x)fh + \\frac{1}{2}f''(x)h^2 - \\frac{1}{3!}f'''(x)h^3 + \\frac{1}{4!}f''''(x)h^4 + \\cdots$$\n",
    "\n",
    "Se tiene que $N(h) = f'(x) + \\frac{h^2}{3!}f'''(x) + \\frac{h^4}{5!}f^{(5)}(x) +\\cdots$, luego $m=2$\n",
    "\n",
    "Para simplificar los cálulos, se hace un cálculo tabular de todas estas sucesiones. Para ello se hacen los cálculos de la siguiente manera:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "h & N(h) & N_1(h) & N_2(h) & N_3(h) \\\\\n",
    "rh & N(rh) & N_1(rh) & N_2(rh) & \\cdots \\\\\n",
    "r^2h & N(r^2h) & N_1(r^2h) & \\cdots & \\cdots \\\\\n",
    "r^3h & N(r^3h) & \\cdots & \\cdots & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_richardson(f, x0=0, r=0.5, h=0.5, x_var=symbols('x'), h_var=symbols('h'), Nf=None, m=None, grado=3):\n",
    "    if Nf is None:\n",
    "        Nf = (f.subs(x_var, x0 + h_var) - f.subs(x_var, x0 - h_var))/(2 * h_var)\n",
    "        m = 2\n",
    "        \n",
    "    matriz_r = zeros(grado, grado)\n",
    "    \n",
    "    # primero llenamos la matriz con los valores N(rih)\n",
    "    for row in range(grado):\n",
    "        rih = r ** row * h\n",
    "        matriz_r[row, 0] = nsimplify(Nf.subs(h_var, rih),tolerance=1e-10,rational=True)\n",
    "        \n",
    "    for col in range(1, grado):\n",
    "        for row in range(grado - col):\n",
    "            r_exp = m + col - 1\n",
    "            matriz_r[row, col] = nsimplify((r**r_exp * matriz_r[row, col - 1] - matriz_r[row+1, col - 1])/(r**r_exp - 1), rational=True)\n",
    "            \n",
    "    \n",
    "    return Nf, matriz_r\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EJERCICIO 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sin(2*pi*x)\n",
    "x0, r, h = 0, 0.5, 0.5\n",
    "\n",
    "Nf, matriz_r = derivada_richardson(f=f, x0=x0, r=r, h=h, grado=3)\n",
    "Nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EJEMPLO de HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = -cos(x)\n",
    "x0, r, h = 1, 0.5, 1\n",
    "\n",
    "Nf, matriz_r = derivada_richardson(f=f, x0=x0, r=r, h=h, grado=6)\n",
    "N(matriz_r)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "ANMI.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
